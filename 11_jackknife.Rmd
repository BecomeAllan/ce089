---
title: "Jackknife"
# title: "Métodos de reamostragem"
# subtitle: "Jackknife"
author: "Fernando P. Mayer"
bibliography: ref.bib
output:
  html_document:
    number_sections: true
    toc_depth: 3
---

```{r, cache=FALSE, include=FALSE}
source("setup_knitr.R")
opts_chunk$set(fig.path = "figures/11_jackknife/")
```

# Introdução

- Canivete suiço.
- Equipado com várias ferramentas, fácil transporte.
- Mas ferramentas especializadas são melhores que as do canivete.
- Proposto por Tukey.
- É um procedimento não paramétrico pois nenhuma suposição é feita
sobre a distribuição dos dados.
- É facilmente automatizável.  Um único algoritmo pode ser escrito
tendo como argumentos os dados e a estatística de interesse.
- O método é baseado em amostras de tamanho $n - 1$. Existe a
suposição implicita de comportamento suave com o tamanho da amostra.
- Ao contrário do bootstrap, é um procedimento **determinístico**, ou
  seja, os resultados de um Jackknife sempre serão os mesmos para a
  mesma amostra.

O Jackknife é um procedimento do tipo *leave-one-out* (um caso
particular de validação cruzada).

Seja $x = (x_1, \ldots, x_n)$ uma amostra observada, e defina a
$i$-ésima amostra de jackkinfe $x_{(i)}$ como o subconjunto de $x$, que
"deixa de fora" a $i$-ésima observação, ou seja,
$$
x_{(i)} = (x_1, \ldots, x_{i-1}, x_{i+1}, \ldots, x_n)
$$
Se $\hat{\theta} = T_n(x)$, então a $i$-ésima estimativa de Jackknife é
$\hat{\theta}_{(i)} = T_{n-1}(x_{(i)}), i = 1 \ldots, n$.

Suponha que o parâmetro $\theta = T(F)$ é uma função da distribuição
$F$. Se $F_n$ é a função (de distribuição) empírica de uma amostra de
$F$, então o estimador *plug-in* de $\theta$ é $\hat\theta = T(F_n)$.
Um estimador "*plug-in*" $\hat\theta$ é "suave" (*smooth*) no sentido de
que pequenas mudanças nos dados correspondem a pequenas mudaças em
$\hat\theta$. Por exemplo, a média amostral é um estimador *plug-in*
para a média populacional, mas a mediana amostral **não é** um estimador
*plug-in* para a mediana populacional.

Se $\hat\theta$ é um estimador "suave" para $\theta$, então
$$
\hat\theta_{(i)}^{\star} = (F_{n-1}(x_{(i)}))
$$
é chamada de **estimativa parcial** para $\theta$, e a média das
estimativas parciais, ou seja,
$$
  \hat{\theta}_{(\cdot)}^{\star} = \frac{1}{n} \sum_{i = 1}^{n}
  \hat{\theta}_{(i)}^{\star}
$$
é um estimador pontual para $\hat\theta$.

O erro-padrão de Jackknife para a estimativa pontual
$\hat{\theta}_{(\cdot)}^{\star}$ é portanto

$$
\text{EP}_{jack} = \sqrt{\frac{n-1}{n} \sum_{i=1}^n
  (\hat\theta_{(i)}^{\star} - \hat\theta_{(\cdot)}^{\star})^2}
$$

Isso faz com que o algoritmo de Jackknife seja facilmente automatizado,
funcionando para qualquer estimador que seja uma função suave dos dados.
O termo $\frac{n-1}{n}$ é um fator de correção de viés para as
estimativas de Jackknife.

Nesse mesmo sentido, assumindo independência entre os valores
$\hat\theta_{(i)}^{\star}$, um intervalo de confiança **aproximado** de
$100(1-\alpha)\%$ para $\theta$ pode ser definido como
$$
\hat{\theta}_{(\cdot)}^{\star} \pm t_{\alpha/2,n-1} \text{EP}_{jack}
$$

## Exemplo: média amostral

```{r}
## Simula valores de uma normal
set.seed(123)
x <- rnorm(30, 100, 5)
(n <- length(x))
(xbar <- mean(x))
(s2 <- var(x))
## Erro padrão da média amostral
(ep <- sqrt(s2/n))

## Estimativas parciais de Jackknife
theta.jack <- numeric(n)
for(i in 1:n) {
    theta.jack[i] <- mean(x[-i])
}
theta.jack

## Nesse caso, a mẽdia das estimativas parciais será igual a média da
## amostra
mean(theta.jack)
xbar # da amostra

## Erro padrão
## Usando estimador baseado nas estimativas parciais
sqrt(((n - 1)/n) * sum((theta.jack - mean(theta.jack))^2))
## Essa foma acima foi feita para concordar com
sqrt(var(x)/n)
## quando \hat\theta é a média \bar{x}
```

## Exemplo: desvio padrão amostral

Exemplo 2.1 do Manly. Estimativa do erro padrão para o desvio padrão de
uma VA $X \sim \text{Exp}(1)$, ou seja,  $E[X] = 1$ e $Var[X] = 1$.

```{r}
x <- c(3.56, 0.69, 0.1, 1.84, 3.93, 1.25, 0.18, 1.13, 0.27, 0.5, 0.67,
       0.01, 0.61, 0.82, 1.7, 0.39, 0.11, 1.2, 1.21, 0.72)
hist(x)
(n <- length(x))
(xbar <- mean(x))
## Desvio padrão amostral
(sx <- sd(x))

## Queremos agora obter uma estimativa do erro padrão para o desvio
## padrão amostral

## Estimativas parciais
theta.jack <- numeric(n)
for(i in 1:n) {
    theta.jack[i] <- sd(x[-i])
}
theta.jack
mean(theta.jack)
sx # amostral

## Erro padrão
## Usando estimador baseado nas estimativas parciais
(ep.jack <- sqrt(((n - 1)/n) * sum((theta.jack - mean(theta.jack))^2)))
```

Comparando com Bootstrap

```{r}
## Definições
B <- 2000
R <- numeric(B)
## Bootstrap para a estimativa do erro padrão do R (correlação amostral)
for (b in 1:B) {
    i <- sample(1:n, size = n, replace = TRUE)
    R[b] <- sd(x[i])
}
## Resultado
mean(R)
mean(theta.jack)
(se.R <- sd(R))
ep.jack
```

<div class="panel panel-primary">
<div class="panel-heading">Pseudo-valores</div>
<div class="panel-body">
A ideia por trás do Jackknife pode ser entendida com base na definição
de pseudo-valores, que é fundamentada no estimador da média amostral
$$
  \bar{X} = \frac{1}{n} \sum_{i=1}^{n} X_i.
$$

A média com a $i$-ésima observação removida, $X_{(i)}$, é
$$
  \bar{X}_{(i)} = \frac{1}{n - 1}
  \left( \sum_{i=1}^{n} X_i  - X_{i} \right)
$$

Combinando as expressões anteriores, pode-se determinar o valor de $X_i$
por
$$
  X_i = n\bar{X} - (n - 1) \bar{X}_{(i)}.
$$

Essa expressão não tem valor para o caso da média, que serviu apenas de
inspiração. Mas tem utilidade para outros parâmetros/estatísticas.

Suponha que $\theta$ seja um parâmetro a ser estimado a partir de uma
função dos dados (amostra de tamanho $n$)
$$
  \hat{\theta} = T(X_1, X_2, \ldots, X_n).
$$

A quantidade
$$
  \hat{\theta}_i^{\star} = n \hat{\theta} - (n - 1) \hat{\theta}_{(i)}
$$

é denominada de **pseudo-valor** e se baseia nas diferença ponderadas
entra a estimativa ($\hat{\theta}$) e todas as observações e na
**estimativa parcial**, ou seja, aquela sem a $i$-ésima observação
($\hat{\theta}_{(i)}$).

O estimador pontual de Jackknife é definido por
$$
  \hat{\theta}^{\star} = \frac{1}{n} \sum_{i = 1}^{n}
  \hat{\theta}_{i}^{\star},
$$
ou seja, é a **média dos pseudo-valores**.

Os valores $\hat{\theta}$ e $\hat{\theta}^{\star}$ não são
necessariamente iguais nos casos gerais.

Se for assumido que os valores $\hat\theta_{i}^{\star}$, $i = 1, \ldots,
n$, são independentes, a variância do estimador de Jackknife é dada por
$$
\text{Var}(\hat{\theta}^{\star}) = \frac{s^2}{n},
  \quad s^2 = \frac{1}{n - 1}
  \sum_{i = 1}^n (\hat{\theta}_{i}^{\star} - \hat{\theta}^{\star})^2.
$$
Portanto o erro padrão será
$$
\text{EP}_{jack} = \sqrt{\text{Var}(\hat{\theta}^{\star})} =
\sqrt{\frac{s^2}{n}}
$$

#### Exemplo: média amostral {-}

```{r}
## Simula valores de uma normal
set.seed(123)
(x <- rnorm(30, 100, 5))
(n <- length(x))
(xbar <- mean(x))
(s2 <- var(x))
(ep <- sqrt(s2/n))

## Estimativas parciais
theta.jack <- numeric(n)
for(i in 1:n) {
    theta.jack[i] <- mean(x[-i])
}
theta.jack

## Conceito: reproduzir o valor
x[1]
n * xbar - (n - 1) * theta.jack[1]

## Pseudo valores
(pv <- n * xbar - (n - 1) * theta.jack)
mean(pv)
xbar # média da amostra
## É o mesmo que
mean(theta.jack)

## Erro padrão
## Usando os pseudo-valores
sqrt(var(pv)/n)
ep # erro padrão da amostra
## Conta "na mão"
sqrt((sum((pv - mean(pv))^2)/(n - 1))/n)

## Usando estimador baseado nas estimativas parciais
sqrt(((n - 1)/n) * sum((theta.jack - mean(theta.jack))^2))

## Resultados: quando o estimador for a média amostral, os
## pseudo-valores serão iguais aos valores observados
round(cbind(Amostra = x, "Pseudo-valores" = pv,
            "Estimativas parciais" = theta.jack), 2)
```

#### Exemplo: desvio-padrão amostral {-}


```{r}
x <- c(3.56, 0.69, 0.1, 1.84, 3.93, 1.25, 0.18, 1.13, 0.27, 0.5, 0.67,
       0.01, 0.61, 0.82, 1.7, 0.39, 0.11, 1.2, 1.21, 0.72)
(n <- length(x))
(xbar <- mean(x))
(sx <- sd(x))

## Estimativas parciais
theta.jack <- numeric(n)
for(i in 1:n) {
    theta.jack[i] <- sd(x[-i])
}
theta.jack

## NOTE que agora os valores não serão necessariamente iguais (isso só
## ocorre quando a estatística é a média amostral)
x[1]
n * sx - (n - 1) * theta.jack[1]

## Pseudo valores
(pv <- n * sx - (n - 1) * theta.jack)
## Estimativa pontual de Jackknife para o DESVIO-PADRÃO
mean(pv)
## Desvio padrão amostral
sx
## Não serão iguais necessariamente
mean(theta.jack)

## Erro padrão
## Usando os pseudo-valores
sqrt(var(pv)/n)

## Usando estimador baseado nas estimativas parciais
sqrt(((n - 1)/n) * sum((theta.jack - mean(theta.jack))^2))

## Resultados: os pseudo-valores não serão iguais aos valores (como
## aconteceu com a mẽdia amostral)
round(cbind(Amostra = x, "Pseudo-valores" = pv,
            "Estimativas parciais" = theta.jack), 2)
```
</div>
</div>


## Exemplo: correlação

```{r, include=FALSE, eval=FALSE}
url <-  "https://web.stanford.edu/~hastie/CASI_files/DATA/kidney.txt"
da <- read.table(url, header = TRUE)
str(da)
plot(da)
cor(da)
(rho <- cor(da$age, da$tot))
n <- nrow(da)

theta.jack <- numeric(n)
for(i in 1:n) {
    theta.jack[i] <- cor(da$age[-i], da$tot[-i])
}
theta.jack

mean(theta.jack)
sqrt(((n - 1)/n) * sum((theta.jack - mean(theta.jack))^2))

(pv <- n * rho - (n - 1) * theta.jack)
mean(pv)
sqrt(var(pv)/n)

```

Exemplo da aula anterior

```{r}
## Estimativa de erro padrão via bootstrap (para comparação)
data(law, package = "bootstrap")
str(law)
plot(law$LSAT, law$GPA)
(rho <- cor(law$LSAT, law$GPA))
## Definições
B <- 2000
n <- nrow(law)
R <- numeric(B)
## Bootstrap para a estimativa do erro padrão do R (correlação amostral)
for (b in 1:B) {
    i <- sample(1:n, size = n, replace = TRUE)
    LSAT <- law$LSAT[i]
    GPA <- law$GPA[i]
    R[b] <- cor(LSAT, GPA)
}
## Resultado
mean(R)
(se.R <- sd(R))

## Usando Jackknife
theta.jack <- numeric(n)
for(j in 1:n) {
    theta.jack[j] <- cor(law$LSAT[-j], law$GPA[-j])
}
## Estimativas parciais
theta.jack
mean(theta.jack)

## Pseudo valores
## Note que alguns valores estão fora do intervalo [-1,1]
(pv <- n * rho - (n - 1) * theta.jack)
mean(pv)
rho # valor da amostra

## Erro padrão
sqrt(var(pv)/n)
se.R # via bootstrap
(ep.jack <- sqrt(((n - 1)/n) * sum((theta.jack - mean(theta.jack))^2)))

## Intervalo de confiança Jackknife (supõe independência e normalidade).
mean(pv) + qt(c(.025, .975), df = n - 1) * sqrt(var(pv)/n)
mean(theta.jack) + qt(c(.025, .975), df = n - 1) * ep.jack
cor.test(law$LSAT, law$GPA)$conf.int # teórico
```

## Exemplo: razão de médias

Exemplo da aula anterior

```{r}
data(patch, package = "bootstrap")
n <- nrow(patch)
y <- patch$y
z <- patch$z
(theta.hat <- mean(y)/mean(z))

## Estimativas parciais: Jackknife
theta.jack <- numeric(n)
for (i in 1:n) {
    theta.jack[i] <- mean(y[-i])/mean(z[-i])
}
mean(theta.jack)
## Erro padrão de Jackknife
(ep.jack <- sqrt(((n - 1)/n) * sum((theta.jack - mean(theta.jack))^2)))

## Via bootstrap
B <- 2000
theta.b <- numeric(B)
for (b in 1:B) {
    i <- sample(1:n, size = n, replace = TRUE)
    y <- patch$y[i]
    z <- patch$z[i]
    theta.b[b] <- mean(y)/mean(z)
}
## Estimativas
(se <- sd(theta.b))

## Intervalos de confiança para a estimativa
## Via bootstrap
theta.boot <- function(dat, ind) {
    y <- dat[ind, 1]
    z <- dat[ind, 2]
    mean(y)/mean(z)
}
dat <- cbind(patch$y, patch$z)
library(boot)
boot.obj <- boot(dat, statistic = theta.boot, R = 2000)
boot.ci(boot.obj, type = c("basic", "norm", "perc"))
## Via Jackknife
mean(theta.jack) + qt(c(.025, .975), df = n - 1) * ep.jack
```

## Exemplo: mediana

```{r, include=FALSE, eval=FALSE}
## Example 8.8 (Failure of Jackknife)
set.seed(123) #for the specific example given
n <- 10
(x <- sample(1:100, size = n))
x <- c(29, 79, 41, 86, 91, 5, 50, 83, 51, 42)
## Jackknife estimate of se
M <- numeric(n)
for (i in 1:n) {        #leave one out
    y <- x[-i]
    M[i] <- median(y)
}
Mbar <- mean(M)
sqrt((n - 1)/n * sum((M - Mbar)^2))

## bootstrap estimate of se
Mb <- replicate(1000, expr = {
    y <- sample(x, size = n, replace = TRUE)
    median(y)
})
sd(Mb)
```

Este é um exemplo de quando o método de Jackknife pode **falhar**. Isso
ocorre quando $\hat\theta$ **não é uma função "suave"** da amostra, como
é o caso da mediana.

```{r}
## Exemplo extremo
x <- c(29, 79, 41, 86, 91, 5, 50, 83, 51, 42)
median(x)
n <- length(x)
## Erro padrão via Jackknife
M <- numeric(n)
for (i in 1:n) {
    y <- x[-i]
    M[i] <- median(y)
}
(Mbar <- mean(M))
sqrt((n - 1)/n * sum((M - Mbar)^2))

## Erro padrão via bootstrap
Mb <- replicate(1000, expr = {
    y <- sample(x, size = n, replace = TRUE)
    median(y)
})
mean(Mb)
sd(Mb)
```

# Validação cruzada

A validação cruzada é um método de particionamento de dados que pode ser
usado para verificar:

- A estabilidade de estimativas de parâmetros
- A acurácia de algoritmos de classificação
  - O modelo é estimado no conjunto de "treinamento", e verificado no
    conjunto de "teste"
- A adequabilidade de um modelo ajustado
  - O ajuste de qualquer modelo pode ser verificado também através de um
    conjunto de teste

O Jackknife pode ser considerado como um caso particular de validação
cruzada, onde o particionamento dos dados é feito um a um.

Outra forma de validação cruzada é a *n-fold*, que particiona os dados
em $n$ conjuntos de teste:

- Esse procedimento *leave-one-out* é como o Jackknife
- Os dados podem ser divididos em qualquer número $K$ de partições,
portanto haverão $K$ bases de teste, e o modelo será ajustado $K$
vezes

Algoritmo *n-fold* para validação cruzada para o **erro de predição**

1. Para $k = 1, \ldots, n$, seja $(x_k, y_k)$ os pontos de teste e use
   as observações restantes para ajustar o modelo
    a. Ajuste o modelo para as $n-1$ observações da base de "teste"
   $(x_i, y_i)$, $i \neq k$
    b. Calcule o velor predito $\hat{y}_k = \hat{\beta}_0 +
   \hat{\beta}_1 x_k$ para o **ponto de teste**
    c. Calcule o erro predito $e_k = y_k - \hat{y}_k$
2. Calcule o erro quadrático médio $\hat{\sigma}^{2}_e =
   \frac{1}{n} \sum_{k=1}^n e_k^2$


```{r}
## Dados
data("ironslag", package = "DAAG")
str(ironslag)
plot(magnetic ~ chemical, ironslag)

## Modelos propostos
a <- seq(10, 40, .1) # sequencia para graficos
## Linear
par(mfrow = c(2, 2))
L1 <- lm(magnetic ~ chemical, ironslag)
plot(magnetic ~ chemical, ironslag, main = "Linear", pch = 19)
yhat1 <- L1$coef[1] + L1$coef[2] * a
lines(a, yhat1, lwd = 2, col = 2)
## Quadratico
L2 <- lm(magnetic ~ chemical + I(chemical^2), ironslag)
plot(magnetic ~ chemical, ironslag, main = "Quadratic", pch = 19)
yhat2 <- L2$coef[1] + L2$coef[2] * a + L2$coef[3] * a^2
lines(a, yhat2, lwd = 2, col = 2)
## Exponencial
L3 <- lm(log(magnetic) ~ chemical, ironslag)
plot(magnetic ~ chemical, ironslag, main = "Exponential", pch = 19)
logyhat3 <- L3$coef[1] + L3$coef[2] * a
yhat3 <- exp(logyhat3)
lines(a, yhat3, lwd = 2, col = 2)
## log-log
L4 <- lm(log(magnetic) ~ log(chemical), ironslag)
plot(log(magnetic) ~ log(chemical), ironslag,
     main = "Log-Log", pch = 19)
logyhat4 <- L4$coef[1] + L4$coef[2] * log(a)
lines(log(a), logyhat4, lwd = 2, col = 2)

## Validação cruzada
n <- length(ironslag$magnetic)
e1 <- e2 <- e3 <- e4 <- numeric(n)
for (k in 1:n) {
    y <- ironslag$magnetic[-k]
    x <- ironslag$chemical[-k]
    ## Linear
    J1 <- lm(y ~ x)
    yhat1 <- J1$coef[1] + J1$coef[2] * ironslag$chemical[k]
    e1[k] <- ironslag$magnetic[k] - yhat1
    ## Quadrático
    J2 <- lm(y ~ x + I(x^2))
    yhat2 <- J2$coef[1] + J2$coef[2] * ironslag$chemical[k] +
        J2$coef[3] * ironslag$chemical[k]^2
    e2[k] <- ironslag$magnetic[k] - yhat2
    ## Exponencial
    J3 <- lm(log(y) ~ x)
    logyhat3 <- J3$coef[1] + J3$coef[2] * ironslag$chemical[k]
    yhat3 <- exp(logyhat3)
    e3[k] <- ironslag$magnetic[k] - yhat3
    ## Log-log
    J4 <- lm(log(y) ~ log(x))
    logyhat4 <- J4$coef[1] + J4$coef[2] * log(ironslag$chemical[k])
    yhat4 <- exp(logyhat4)
    e4[k] <- ironslag$magnetic[k] - yhat4
}

## Estimativas do erro quadrático médio
c(mean(e1^2), mean(e2^2), mean(e3^2), mean(e4^2))
```

```{r, out.width='100%'}
## Comparação dos modelos
par(mfrow = c(4, 2))
plot(L1$fit, L1$res, main = "Linear"); abline(0, 0)
qqnorm(L1$res); qqline(L1$res)
plot(L2$fit, L2$res, main = "Quadrático"); abline(0, 0)
qqnorm(L2$res); qqline(L2$res)
plot(L3$fit, L3$res, main = "Exponencial"); abline(0, 0)
qqnorm(L3$res); qqline(L3$res)
plot(L4$fit, L4$res, main = "log-log"); abline(0, 0)
qqnorm(L4$res); qqline(L4$res)
par(mfrow = c(1, 1))
```
