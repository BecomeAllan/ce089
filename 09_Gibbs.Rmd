---
title: "Métodos de Monte Carlo via Cadeias de Markov"
subtitle: "Amostrador de Gibbs"
author: "Fernando P. Mayer"
bibliography: ref.bib
output:
  html_document:
    number_sections: true
    toc_depth: 3
---

```{r, cache=FALSE, include=FALSE}
source("setup_knitr.R")
opts_chunk$set(fig.path = "figures/09_Gibbs/")
```

# Introdução

Amostrador de Gibbs multi-estágios

Seja $X = (X_1, X_2, \ldots, X_d)$ um vetor $d$-dimensional de variáveis
aleatórias. Podemos denotar o vetor aleatório $d-1$ dimensional como

$$
X_{(-j)} = (X_1, \ldots, X_{j-1}, X_{j+1}, \ldots, X_d)
$$

ou seja, o vetor $X$ **excluindo** o elemento na posição $j$. Além
disso, suponha que podemos simular das correspondentes **densidades
condicionais univariadas**, $f_1, f_2, \ldots, f_d$, ou seja, podemos
amostrar de

$$
\begin{aligned}
X_j|X_{(-j)} \, &\sim \, f_j(X_j|X_{(-j)}) \\
X_j|x_1, \ldots, x_{j-1}, x_{j+1}, \ldots, x_d
   \, &\sim \, f_j(X_j|x_1, \ldots, x_{j-1}, x_{j+1}, \ldots, x_d)
\end{aligned}
$$

## Exemplo (normal bivariada)

```{r}
##======================================================================
## Exemplo de Rizzo, pg. 320
## Example 11.10 (Gibbs sampler: Bivariate distribution)

## Define constantes
N <- 5000
## Burnin
burn <- 1000
## Matriz para armazenar as amostras
X <- matrix(0, N, 2)

## Define parametros da Normal bivariada
rho <- -.75
mu1 <- 0
mu2 <- 2
sigma1 <- 1
sigma2 <- .5
s1 <- sqrt(1-rho^2)*sigma1
s2 <- sqrt(1-rho^2)*sigma2

## Gera a cadeia

## Valores iniciais
X[1, ] <- c(mu1, mu2)

for (i in 2:N) {
    x2 <- X[i-1, 2]
    m1 <- mu1 + rho * (x2 - mu2) * sigma1/sigma2
    X[i, 1] <- rnorm(1, m1, s1)
    x1 <- X[i, 1]
    m2 <- mu2 + rho * (x1 - mu1) * sigma2/sigma1
    X[i, 2] <- rnorm(1, m2, s2)
}

## Cadeias (mude os valores iniciais para ver convergencia)
matplot(X, type = "l")
## Correlacao entre os valores
acf(X[,1])
acf(X[,2])
## Conjunta
plot(X, main = "", xlab = bquote(X[1]),
     ylab = bquote(X[2]), ylim = range(X[, 2]))

## Descarta os primeiros 1000 valores
b <- burn + 1
x <- X[b:N, ]

matplot(x, type = "l")
## Nao elimina o problema de autocorrelacao
acf(x[,1])
acf(x[,2])
## Conjunta
plot(x, main = "", xlab = bquote(X[1]),
     ylab = bquote(X[2]), ylim = range(x[, 2]))

## Compara estatisticas
colMeans(x)
cov(x)
cor(x)
##======================================================================
```

## Exemplo (beta-binomial)

```{r}
##======================================================================
## Exemplo 7.2 do Casella e Robert - Beta-binomial
## X | theta ~ Bin(n, theta)
## theta ~ Beta(a, b)

## Condicionais são
## X | theta ~ Bin(n, theta)
## theta | X ~ Beta(x + a, n - x + b)

## Define constantes
N <- 5000
## Vetores para armazenar as amostras
T <- numeric(N)
X <- numeric(N)

## Aqui da para testar varios valores para ver o impacto da priori
a <- 3
b <- 7
curve(dbeta(x, a, b), from = 0, to = 1)

## Define valores
n <- 15
## Valores iniciais
T[1] <- rbeta(1, a, b)
X[1] <- rbinom(1, n, T[1])

## Amostrador de Gibbs
for (i in 2:N) {
    X[i] <- rbinom(1, n, T[i - 1])
    T[i] <- rbeta(1, X[i] + a, n - X[i] + b)
}

## Cadeias (mude os valores iniciais para ver convergencia)
par(mfrow = c(2, 1))
plot(X, type = "l")
plot(T, type = "l")
par(mfrow = c(1, 2))

## Correlacao entre os valores
acf(X)
acf(T)
par(mfrow = c(1, 1))
## Conjunta
plot(X, T)

## Integrando a conjunta em relação a theta, chega na marginal de X, que
## é uma Beta-Binomial
betabinom <- function(x, a, b, n) {
    choose(n, x) * (gamma(a + b)/(gamma(a) * gamma(b))) *
        ((gamma(x + a) * gamma(n - x + b))/
        gamma(a + b + n))
}

## Compara amostra com distribuicoes teoricas
par(mfrow = c(1, 2))
## A Beta-binomial é uma distribuição discreta
plot(prop.table(table(X)), type = "h")
points((0:14) + 0.2,
       betabinom(x = 0:14, a = a, b = b, n = n), type = "h", col = 2)
hist(T, freq = FALSE, main = "", xlab = expression(theta))
curve(dbeta(x, a, b), add = TRUE, col = 2)
par(mfrow = c(1, 1))

## Fazendo o thinning
x <- X[seq(1, length(X), 5)]
t <- T[seq(1, length(T), 5)]

## Correlacao entre os valores
par(mfrow = c(1, 2))
acf(x)
acf(t)
par(mfrow = c(2, 1))

## Cadeias (mude os valores iniciais para ver convergencia)
par(mfrow = c(2, 1))
plot(x, type = "l")
plot(t, type = "l")
par(mfrow = c(1, 1))

## Conjunta
plot(x, t)

## Compara amostra com distribuicoes teoricas
par(mfrow = c(1, 2))
## A Beta-binomial é uma distribuição discreta
plot(prop.table(table(x)), type = "h")
points((0:14) + 0.2,
       betabinom(x = 0:14, a = a, b = b, n = n), type = "h", col = 2)
hist(t, freq = FALSE, main = "", xlab = expression(theta))
curve(dbeta(x, a, b), add = TRUE, col = 2)
par(mfrow = c(1, 1))
##======================================================================
```

## Exemplo (Poisson-Gama - baleia)

```{r}
##======================================================================
## Exemplo da baleia (ver slides)
x <- 10
n <- 15
teta <- seq(0, 2, length = 200)
alfa <- 1
beta <- 0.1
## Calcula a densidade da priori
priori.ni <- dgamma(teta, alfa, beta)
(alfa.star <- alfa + x)
(beta.star <- beta + n)

post.ni <- dgamma(teta, alfa.star, beta.star)
plot(teta, post.ni, type = "l", xlab = expression(theta),
     ylab = "Densidade de probabilidade")
lines(teta, priori.ni, lty = 2)
lines(teta, dgamma(teta, 1 + x, n), col = 2, lty = 2)
legend("topright",  lty = c(1, 2, 2), col = c(1, 1, 2),
       legend = c("Posterior", "Priori", "Verossimilhança"))

## Dados
library(runjags)
datalist <- dump.format(list(x = x, n = n))
params <- c("theta")
inicial <- dump.format(list(theta = 0.5))

## Modelo
mod <- "model{
x ~ dpois(n * theta)
theta ~ dgamma(1, 0.1)
}"

## Ajuste
m.jags <- run.jags(
    model = mod, monitor = params, data = datalist, inits = c(inicial,inicial),
    n.chains = 2, burnin = 5000, thin = 5, sample = 10000
)

## Resultados
m.jags
qgamma(c(0.025, 0.5, 0.975), alfa.star, beta.star)
plot(m.jags)
```

```{r}
## Usando prioris informativas
alfa.i <- 4.5
beta.i <- 10
## Calcula a densidade da priori
priori.i <- dgamma(teta, alfa.i, beta.i)
alfa.star.i <- alfa.i + x
beta.star.i <- beta.i + n
## Cálculo da densidade da posterior com a priori informativa
post.i <- dgamma(teta, alfa.star.i, beta.star.i)
## Visualização
plot(teta, post.i, type = "l", xlab = expression(theta),
     ylab = "Densidade de probabilidade")
lines(teta, priori.i, lty = 2)
lines(teta, dgamma(teta, 1 + x, n), col = 2, lty = 2)
legend("topright",  lty = c(1, 2, 2), col = c(1, 1, 2),
       legend = c("Posterior", "Priori", "Verossimilhança"))

## Modelo
mod <- "model{
x ~ dpois(n * theta)
theta ~ dgamma(4.5, 10)
}"

## Ajuste
m.jags <- run.jags(
    model = mod, monitor = params, data = datalist, inits = c(inicial,inicial),
    n.chains = 2, burnin = 5000, thin = 5, sample = 10000
)

## Resultados
m.jags
qgamma(c(0.025, 0.5, 0.975), alfa.star.i, beta.star.i)
```

```{r}
plot(m.jags)
```
